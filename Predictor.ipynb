{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "capital-berkeley",
   "metadata": {},
   "source": [
    "# Legacy Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "functioning-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import LayoutLMForSequenceClassification, LayoutLMTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytesseract\n",
    "from datasets import Features, Sequence, ClassLabel, Value, Array2D\n",
    "import numpy as np\n",
    "\n",
    "classes = [\"birth certificate\", \"driving\", \"ssn\", \"tax_document\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-cedar",
   "metadata": {},
   "source": [
    "# Legacy Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "german-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def normalize_box(box, width, height):\n",
    "     return [\n",
    "         int(1000 * (box[0] / width)),\n",
    "         int(1000 * (box[1] / height)),\n",
    "         int(1000 * (box[2] / width)),\n",
    "         int(1000 * (box[3] / height)),\n",
    "     ]\n",
    "\n",
    "def apply_ocr(example):\n",
    "        # get the image\n",
    "        image = Image.open(example['image_path'])\n",
    "\n",
    "        width, height = image.size\n",
    "        \n",
    "        # apply ocr to the image \n",
    "        ocr_df = pytesseract.image_to_data(image, output_type='data.frame')\n",
    "        float_cols = ocr_df.select_dtypes('float').columns\n",
    "        ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
    "        ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\n",
    "        ocr_df = ocr_df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "        ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
    "\n",
    "        # get the words and actual (unnormalized) bounding boxes\n",
    "        #words = [word for word in ocr_df.text if str(word) != 'nan'])\n",
    "        words = list(ocr_df.text)\n",
    "        words = [str(w) for w in words]\n",
    "        coordinates = ocr_df[['left', 'top', 'width', 'height']]\n",
    "        actual_boxes = []\n",
    "        for idx, row in coordinates.iterrows():\n",
    "            x, y, w, h = tuple(row) # the row comes in (left, top, width, height) format\n",
    "            actual_box = [x, y, x+w, y+h] # we turn it into (left, top, left+width, top+height) to get the actual box \n",
    "            actual_boxes.append(actual_box)\n",
    "        \n",
    "        # normalize the bounding boxes\n",
    "        boxes = []\n",
    "        for box in actual_boxes:\n",
    "            boxes.append(normalize_box(box, width, height))\n",
    "        \n",
    "        # add as extra columns \n",
    "        assert len(words) == len(boxes)\n",
    "        example['words'] = words\n",
    "        example['bbox'] = boxes\n",
    "        return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "mathematical-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LayoutLMTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
    "\n",
    "def encode_example(example, max_seq_length=512, pad_token_box=[0, 0, 0, 0]):\n",
    "  words = example['words']\n",
    "  normalized_word_boxes = example['bbox']\n",
    "\n",
    "  assert len(words) == len(normalized_word_boxes)\n",
    "\n",
    "  token_boxes = []\n",
    "  for word, box in zip(words, normalized_word_boxes):\n",
    "      word_tokens = tokenizer.tokenize(word)\n",
    "      token_boxes.extend([box] * len(word_tokens))\n",
    "  \n",
    "  # Truncation of token_boxes\n",
    "  special_tokens_count = 2 \n",
    "  if len(token_boxes) > max_seq_length - special_tokens_count:\n",
    "      token_boxes = token_boxes[: (max_seq_length - special_tokens_count)]\n",
    "  \n",
    "  # add bounding boxes of cls + sep tokens\n",
    "  token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n",
    "  \n",
    "  encoding = tokenizer(' '.join(words), padding='max_length', truncation=True)\n",
    "  # Padding of token_boxes up the bounding boxes to the sequence length.\n",
    "  input_ids = tokenizer(' '.join(words), truncation=True)[\"input_ids\"]\n",
    "  padding_length = max_seq_length - len(input_ids)\n",
    "  token_boxes += [pad_token_box] * padding_length\n",
    "  encoding['bbox'] = token_boxes\n",
    "\n",
    "  assert len(encoding['input_ids']) == max_seq_length\n",
    "  assert len(encoding['attention_mask']) == max_seq_length\n",
    "  assert len(encoding['token_type_ids']) == max_seq_length\n",
    "  assert len(encoding['bbox']) == max_seq_length\n",
    "\n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afraid-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to define the features ourselves as the bbox of LayoutLM are an extra feature\n",
    "features = Features({\n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'token_type_ids': Sequence(Value(dtype='int64')),\n",
    "    'image_path': Value(dtype='string'),\n",
    "    'words': Sequence(feature=Value(dtype='string')),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-legend",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "intense-recall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayoutLMForSequenceClassification(\n",
       "  (layoutlm): LayoutLMModel(\n",
       "    (embeddings): LayoutLMEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (x_position_embeddings): Embedding(1024, 768)\n",
       "      (y_position_embeddings): Embedding(1024, 768)\n",
       "      (h_position_embeddings): Embedding(1024, 768)\n",
       "      (w_position_embeddings): Embedding(1024, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): LayoutLMEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): LayoutLMPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LayoutLMForSequenceClassification.from_pretrained(\"C:/Users/atulp/Desktop/document classifier/layoutclassification/Document-Classification-using-LayoutLM/saved_model\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-credits",
   "metadata": {},
   "source": [
    "# Data Processing Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "involved-cycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data [] ['s1 - Copy (4).jpg']\n",
      "['s1 - Copy (4).jpg']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/s1 - Copy (4).jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image_path\n",
       "0  test_data/s1 - Copy (4).jpg"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "labels = []\n",
    "dataset_path = 'test_data'\n",
    "\n",
    "for label_folder, _, file_names in os.walk(dataset_path):\n",
    "    print(label_folder, _, file_names)\n",
    "    print(file_names)\n",
    "    relative_image_names = []\n",
    "    relative_image_names.append(dataset_path + \"/\" + file_names[0])\n",
    "    images.extend(relative_image_names)\n",
    "test_data = pd.DataFrame.from_dict({'image_path': images})\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "auburn-letter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "updated_test_dataset = test_dataset.map(apply_ocr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "together-techno",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['000-000-0000', 'THIS', 'NUMBER', 'HAS', 'BEE', 'NIESTABLISHED', 'FOR', 'JOHN', 'SMITH', 'SIGNATURE']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(updated_test_dataset)\n",
    "print(len(df[\"words\"][0]))\n",
    "print(df[\"words\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "adjusted-magnitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_test_dataset = updated_test_dataset.map(lambda example: encode_example(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fallen-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_dataset.set_format(type='torch', columns=['input_ids', 'bbox', 'attention_mask', 'token_type_ids'])\n",
    "test_dataloader = torch.utils.data.DataLoader(encoded_test_dataset, batch_size=1, shuffle=True)\n",
    "test_batch = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "accessory-thanksgiving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.7278, -3.1298, -3.0508,  6.4619]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "input_ids = test_batch[\"input_ids\"].to(device)\n",
    "bbox = test_batch[\"bbox\"].to(device)\n",
    "attention_mask = test_batch[\"attention_mask\"].to(device)\n",
    "token_type_ids = test_batch[\"token_type_ids\"].to(device)\n",
    "\n",
    "# forward pass\n",
    "outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, \n",
    "                token_type_ids=token_type_ids)\n",
    "\n",
    "# prediction = int(torch.max(outputs.data, 1)[1].numpy())\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f8399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "documentary-hartford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birth certificate: 0%\n",
      "driving: 0%\n",
      "ssn: 0%\n",
      "tax_document: 100%\n"
     ]
    }
   ],
   "source": [
    "# import torch.nn.functional as F\n",
    "# pt_predictions = F.softmax(outputs[0], dim=-1)\n",
    "# pt_predictions\n",
    "\n",
    "classification_logits = outputs.logits\n",
    "classification_results = torch.softmax(classification_logits, dim=1).tolist()[0]\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(classification_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "unsigned-luther",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'birth certificate': '0%', 'driving': '0%', 'ssn': '0%', 'tax_document': '100%'}\n"
     ]
    }
   ],
   "source": [
    "thisdict ={}\n",
    "for i in range(len(classes)):\n",
    "    thisdict[classes[i]] = str(int(round(classification_results[i] * 100))) + \"%\"\n",
    "print(thisdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "color-bones",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7737e-04, 6.8262e-05, 7.3876e-05, 9.9958e-01]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "pt_predictions = F.softmax(outputs[0], dim=-1)\n",
    "pt_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "helpful-seventh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "extra-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NATIVE T5\n",
    "\n",
    "# generated_answer = model.generate(input_ids, attention_mask=attention_mask, \n",
    "#                                  max_length=decoder_max_len, top_p=0.98, top_k=50)\n",
    "# decoded_answer = tokenizer.decode(generated_answer.numpy()[0])\n",
    "# print(\"Answer: \", decoded_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "brilliant-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirementsss.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba18c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
